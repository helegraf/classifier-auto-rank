\chapter{Evaluation}
\label{sec:evaluation}

This chapter is dedicated to the evaluation of the proposed ranking approaches. First, the oracle and baseline against which the rankers are compared are explained. Second, the experimental setup, that is conditions under which the evaluation was conducted, is set forth. Last, results are presented and discussed.

\section{Baseline and Oracle}
% Workings of oracle
In order to evaluate the implemented rankers, their outputs are compared to a perfect output generated by an oracle. The oracle is implemented as a ranker that, after having been trained on a meta data-performance data set, returns correct rankings when queried for any instance of that data set. The correct ranking is implied by the performance values (highest first, as the measure used here is predictive accuracy), with ties being handled in a sequential manner, i.e. for the same performance values, the algorithm that was encountered first when constructing the ranking will be ranked before all others with the same value.

% Evaluation Measures used & computed
As rankings returned by this oracle represent the ideal solution, the predicted rankings are compared with them through the measures presented in chapter \ref{sec:fundamentals}. These include the Kendall rank correlation coefficient, performance loss and best three loss regarding classifier performance. For all three measures, it is computed whether any difference in performance is significant, by means of the p-value for the Mann-Whitney U. The regression-based rankers pose a special case: internally, they predict a performance value for each classifier, which therefore can be compared to the actual performance values to show how well they are predicted. This is done by computing the root mean square error between the predicted and actual performance values. In summary, it is desirable for the rankers to come as close as possible to the correct ranking, which is reflected by a high rank correlation and a low loss and root mean square error.

% Baseline
Furthermore, we are interested in the question as to what degree knowing about the properties of a data set influences the quality of rankings. Therefore, a ranker that is agnostic to the meta features of a query data set, meaning that it will always return the same ranking for any data set, is used as a baseline. More specifically, a best algorithm strategy that iteratively determines a ranking by counting the number of data sets where an algorithm is the best choice, is implemented. That means the first algorithm in the best algorithm ranking is the classifier that performs best on most data sets, the second is the best on most data sets when only rankings excluding the first algorithm are considered, and so forth. This baseline is evaluated in the same way as the preference rankers; as it does not predict performance values, the root mean square error of predicted performance values cannot be computed. Ideally, rankings returned by the preference and regression based rankers should then be statistically significantly better than the static baseline, as this would indicate a connection of certain meta features and a performance-induced ranking of classifiers for a data set. 

\section{Experimental Setup}
In the following paragraphs, it is briefly summarized under which conditions the experiments which led to the results discussed in the next section were observed. This includes which classifiers and data sets were chosen, and the specifications of the machines the evaluation was executed on. It is described how the evaluation was carried out, and which specific regression-based and preference-based implementations were used.

% Which classifiers 
The classifiers considered in the rankings were the learning algorithms implemented in WEKA that are fit for classification, excluding meta and ensemble methods. The full list of the 22 classifiers can be gathered from Table \ref{tab:bestAlgorithmRanking}. They were all used in their default configurations as specified by WEKA. 

% Which data sets
Data sets for the analysis were gathered from OpenML. From all data sets considered as `active' on OpenML, which yields 2050 data sets that have been approved with no severe issues found for them so far \cite{openMLGuide}, all data sets that comply with the constraints of the learning problem at hand were selected. Only data sets in the .ARFF format with a defined, nominal target feature which were not specifically designed for streaming analysis\footnotemark{} were considered, which resulted in a reduced selection of 812 instances. Since this amount of data sets hindered the evaluation considerably, large data sets with more than 1000 instances or 100 features were removed as well, leading to a final selection of 448 data sets. Even though evaluation was only carried out on the shorter list, both the list of 812 and 448 data sets are included in the supplementary material for completeness.

\footnotetext{Data sets that contained the substring `BNG' in their name for Bayesian Network Generator, which contain data artificially generated by a Bayesian Network \cite{van2014algorithm} for the sake of data stream analysis were not included in the evaluation.}

% How base performance data generated
The performance data of the classifiers for the selected data sets was generated on a linux cluster with nodes consisting of two Intel(R) Xeon(R) E5-2670 processors running at 2.6GHz, 64GB RAM. For the computation of a single performance value of a classifier on a data set, two CPU cores and 16GB RAM were used, with a timeout of eight hours. Up to 150 of such processes were executed in parallel. For each of the classifiers, the predictive accuracy (the percentage of correctly classified instances in the data set) was recorded on each data set by means of five times stratified Monte-Carlo cross validation with a 70/30 split in training and test data. The stratified split was generated with the help of JAICore, `A Java library for infrastructural classes and basic algorithms in the area reasoning, search, and machine learning' [Reference Missing]. When a timeout occurred or the evaluation failed otherwise, the predictive accuracy was set to zero.

% How rest of the evaluation carried out
All other experiments were carried out on a Windows Machine running Windows 10 Education (version 1709) with a Intel(R) Core(TM) i5-4200U CPU running at 1.60GHz (2.30GHz max) with 12 GB RAM. It is notable that especially the time-related results are to be viewed in regard to this setup. No timeouts were used in this evaluation, and the rankers were evaluated on the training data by means of leave-one-out estimation, that is for the n data sets for which performance values were recorded, they were trained with the meta data - performance examples for n - 1 data sets, and queried for the left-out data set. As n = 448 in this case, the detailed values for each data set are not included in the discussion of the results. They can, however, be found in the supplementary material, offering clues to how each ranker performed in the prediction of a ranking for each of the used data sets, identified by their ID on OpenML. Furthermore, the evaluation was carried out twice for all measures: once with the full meta data used, and one time using only the meta features not generated by landmarkers, that is the ones that require probing to be carried out for a new data set.

% Implementation of measures
For the computation of the Mann-Whitney U and Kendall rank correlation coefficient, Apache Commons implementations were used [REFERENCE MISSING]. Regarding the Mann-Whitney U, the strategy for ties was set to averaging, and the NaNs were set to be removed before the analysis was started.

% How find best version of preference and regression ranker
Furthermore, four alternatives were selected for a regression-based and preference-based alternative each, to get a general idea of how well the respective approach might be suited for the problem. For the regression-based approach, the algorithms chosen were random forest, REPTree, M5P, and linear regression. For all of them, the WEKA implemenation \cite{hall2009weka} was used, and the algorithms where used with the standard hyperparameters set by WEKA. As the preference-based ranking falls under the category of label ranking, and in the jPL-framework, which was used for the implementation of the label ranking algorithms and data set representations, only two alternatives for label ranking are implemented \cite{intelligent2017jpl}, the other two alternatives were generated by modifying the hyperparameters. The implemented alternatives are label ranking by pairwise comparison and instance-based label ranking. As in early tests it became apparent that the instance-based approach might hold more potential, for label ranking by pairwise comparison, only the default configuration dictated by jPL was used. For instance-based label ranking, three different configurations were evaluated, the default configurations and two custom ones. For the first customization, the rank aggregation algorithm used by the label ranker was set to Kemeny-Young. For the second customization, additionally, the number of neighbors of the base learner of the label ranker, namely k nearest neighbor (kNN), was set to the square root of the number of instances in the training data set. All of the described rankers are included in an overview in Table \ref{tab:methods}. In the following section, these eight variants are compared with the baseline and oracle mentioned in the previous section, and among each others.

\begin{table}
\resizebox{\textwidth}{!}{%
\begin{tabularx}{1.25\textwidth}{>{\hsize=.4\hsize}X | >{\hsize=1.6\hsize}X}
	Ranker				& Configuration \\ \hline\hline
	LinearRegression		& Linear regression with S:0, R:1.0E-8, num-decimal-places:4 \\ \hline
	M5P					& M5 model trees with M:4 \\ \hline
	RandomForest			& Random forest with P:100, I:100, num-slots:1, K:0, M:1.0, V:0.001, S:1 \\ \hline
	REPTree				& REPT tree with M:2, V:0.001, N:3, S:1, L:1, I: 0.0 \\ \hline \hline	
	InstanceBased		& Instance-based label ranking with rankaggregation: Placket-Luce (norm tolerance: 1E-9, max iterations: 1000, log likelihood tolerance: 1E-5), baselearner: kNN (k: 10) \\ \hline
	InstanceBased		& Instance-based label ranking with rankaggregation: Kemeny-Young, baselearner: kNN (k: 10) \\ \hline
	InstanceBased		& Instance-based label ranking with rankaggregation: Kemeny-Young, baselearner: kNN (k:$\sqrt{\text{number of instances}}$) \\ \hline
	PairwiseComparison 	& Label ranking by pairwise comparison with baselearner: logistic regression (learning rate: 0.001), gradient step: adam (beta1: 0.99, beta2: 0.999, epsilon: 10E-8) \\
\end{tabularx}
}
\caption{An overview over the different base algorithms used in the evaluated rankers.}
\label{tab:methods}
\end{table}

\section{Results}

In the following subsections, results of the evaluation are discussed in detail. First, the quality of the predictions of the rankers is examined. Then, the times required for building the rankers and for predictions made as well as for the calculation of the meta features is reviewed. At last, additional insights gathered during the evaluation are pointed out.

\subsection{Accuracy}

Table \ref{tab:evaluationResults} shows the results of the evaluation, with the best mean values and significant differences emphasized as described in the caption of the table. When looking at these results, it must first be noted that positive findings regarding the Kendall rank correlation can be reported. For the full set of meta data, six out of the eight implemented alternatives outperform the baseline, all of them significantly, as can be learned from Table \ref{tab:significanceResults} which shows the results of the significance tests, with random forest having the highest correlation of 0.495. In contrast, the two approaches that did not overtake the baseline, instance based label ranking in the default configuration and label ranking by pairwise comparison, show a significant disadvantage compared to the baseline, and thus also compared to the other ranking approaches. Regarding performance loss, only linear regression and random forest show a significant advantage, with all label ranking approaches except for instance based label ranking having a significant disadvantage in contrast. Moving on to best three loss, the picture largely remains the same, except that surprisingly, REPTree now also has a significant advantage over the baseline. The lowest value here is achieved by random forest with a best three loss of 1.308, in comparison to 2.440 for the best algorithm baseline. These values are illustrated in Figure \ref{fig:lossGraphics}, with a scatter plot for individual best three loss values on the data sets.

As for the rank correlation most of the rankers had a significant advantage over the baseline, it is interesting to compare them among each other. From Table \ref{tab:evaluationResults} it is clear that the regression rankers are superior to the preference rankers, and this advantage is significant, as the weakest of the regression rankers, REPTree, is still significantly better than the strongest label ranking approach, instance based label ranking with Kemeny-Young rank aggregation ($p \approx 4.305E-6$). Out of the four regression rankers, three approaches are furthermore significantly better than REPTree. 

Compared to the evaluation on the full set of meta data, the results on the reduced set without probing are less distinctive. For the Kendall Rank correlation, linear regression, M5P and random forest are significantly better than the baseline, and for label ranking, both of the instance based approaches with non-default parameters as well, while the other label ranking approaches are significantly worse than the baseline. In total, random forest is still the best alternative with a correlation of 0.439, slightly worse than the correlation achieved on the full meta data. For both loss measures however, while the disadvantages of the label ranking approaches remain, none of the eight alternatives achieves a significantly better loss or best three loss than the baseline anymore.

With reduced meta data, out of the alternatives that pose a significant advantage compared to the baseline, only random forest is significantly better than all other approaches, the others are not significantly different. With loss and best three loss, no such statement can be made.

Summarizing the observed results, in general, a regression-based approach shows an indication to be stronger than a preference-based approach. Specifically, using random forest as a regression model is the best solution observed in the evaluation, and label ranking by pairwise comparison the weakest, as it falls behind the other ranking approaches and the baseline often. A surprising result is that the label ranking approaches achieve very similar results with or without probing.

\input{tables/evaluationResults}

\input{tables/significanceResults}

\tikzsetnextfilename{LossGraphics}
\begin{figure}
%\pgfplotsset{width=\textwidth}
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[
	title=regular scale,
	height=\textwidth,
	width=\textwidth,
	ylabel={BestThreeLoss}, 
	xlabel={Data Set},
]
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,uniaccentblue] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/RandomForestRanker_metaData_small_allPerformanceValues.csv};
\addplot [very thick,mark=none,black,samples=2,domain=0:450]{2.440};
\addplot [very thick,mark=none,uniaccentblue,samples=2,domain=0:450]{1.308};
\addplot [very thick,mark=none,uniaccentblue,samples=2,domain=0:450]{0};
\legend{BestAlgorithm, RandomForest, BestAlgorithm (mean), RandomForest (mean)}
\end{axis}
\end{tikzpicture}%
~%
%
\begin{tikzpicture}
\begin{axis}[
	title=log scale (base 2),
	height=\textwidth,
	width=\textwidth,
	ylabel={BestThreeLoss}, 
	xlabel={Data Set},
	ymode=log,
	log basis y={2}
]
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,uniaccentblue] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/RandomForestRanker_metaData_small_allPerformanceValues.csv};
\addplot [very thick,mark=none,black,samples=2,domain=0:450]{2.440};
\addplot [very thick,mark=none,uniaccentblue,samples=2,domain=0:450]{1.308};
\addplot [very thick,mark=none,uniaccentblue,samples=2,domain=0:450]{0};
\legend{BestAlgorithm, RandomForest, BestAlgorithm (mean), RandomForest (mean)}
\end{axis}
\end{tikzpicture}
}
\caption{The best three loss for the baseline compared to the random forest ranker (full meta data).}
\label{fig:lossGraphics}
\end{figure}


\subsection{Computation Times}

Because the ranking itself is implemented as a means of speeding up the process of algorithm selection, the time it takes to rank is not negligible. The full results of how long it takes to build the respective ranking models and how much time they need for predictions can be taken from Table \ref{tab:times}. Results are rounded to the full millisecond, as due to the times being measured with a stop watch in the code, more accurate measurements are not possible. The times for building the regression rankers are not surprising; the regression models take far more time to be built than most of the label ranking methods, most likely due to the fact the regression based rankers have to train 22 regression models in order to be trained themselves. However, it is notable that while the ranking by Pairwise Comparison was not among the best solutions concerning the accuracy of the results, it is the ranker with the highest mean build time for full meta data, and second highest for a reduced meta data set, whereas in other cases the high quality of results is correlated with a high time invested in the building of the ranker. The Random Forest ranker, for example, is the ranker with the highest build time (apart from Pairwise Comparison), and also with the rankings of the generally highest quality. But the most important fact to be taken away from Table \ref{tab:times} is that the prediction times for all rankers are short enough to be negligible, with the mean prediction time never being higher than five milliseconds and the maximum never exceeding 50 milliseconds. Furthermore, while the build times are higher, these are not as important due to the fact that each model only needs to be trained once. Although in a real-world scenario, calculation times for meta features also have to be taken into account when ranking, which therefore will be discussed in the following paragraph.

\input{tables/times}

Table \ref{tab:metaFeatureTimes} shows the times required for the computation of groups of meta features as discussed in Chapter \ref{sec:approach}. As can be taken from the table, the computation of the full meta features takes approximately 135 milliseconds on average, while the simple meta features without probing add approximately 3 milliseconds to the prediction times on average. Regarding these computation times it is also notable that among the probing meta feature groups, some are considerably more expensive than others, so that in the future it might be sensible to consider only light probing with probing meta feature groups that are not as expensive.

\input{tables/metaFeatureTimes}

\subsection{Further Insights}

Further insights to be noted are the accuracies predicted for classifiers by the regression models, and the static ranking constructed by the best algorithm baseline. The accuracy of the predictions achieved by the regression based rankers are depicted in Table \ref{tab:rootMeanSquareError}, for full and reduced meta data respectively. The first thing that attracts attention in this table is the absurdly high values for Linear Regression and M5P, however, these are all caused by the very high maximum value on one data set with id 685. If these extreme maxima remain out of consideration for Linear Regression and M5P (the other algorithms do not have this problem) more reasonable values are obtained. For M5P, on the full meta data the new maximum is 311.526, mean 8.127 and standard deviation 18.407; on the reduced meta data it is a maximum of 474.692, mean 15.444 and standard deviation 27.433. For Linear Regression, on the full meta data the new maximum is 142.996, mean 9.281 and standard deviation 8.438; on the reduced meta data it is a maximum of 131.456, mean 14.351 and standard deviation 10.299. However, these values are still worse than the ones obtained for the Random Forest and REPTree variants. It comes as little surprise that the Random Forest Ranker also delivers the most accurate results for performance estimates, but as these best values are more than 6\% off on average for the full meta data and even more than 10\% off on average for the reduced meta data, it is questionable whether these prediction can be useful. The usefulness is dependent on the task these predictions would be used for and whether it requires very accurate predictions of performance values.

\input{tables/rootMeanSquareError}

The second interesting thing to consider is the ranking which is created by the best algorithm baseline. This ranking constitutes meta-knowledge about the learning process itself in that it shows which algorithms are generally a good choice for many data sets. This ranking is depicted in Table \ref{tab:bestAlgorithmRanking}. The table is to be read top to bottom; of the 22 algorithms, the algorithm in the i-th row is placed first on most data sets when compared to the next 22-i algorithms. The associated number represents the number of data sets on which the algorithm was placed first. It is, however, not a particularly surprising result, as for example the placement of Random Forest as the first algorithm was to be expected. Random Forest is a learning algorithm that is known to often deliver good performances, which was proven again in the results discussed previously - out of the reviewed ranking approaches, Random Forest was the best choice.

\input{tables/bestAlgorithmRanking}