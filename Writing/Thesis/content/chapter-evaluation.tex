% !TEX root = ../my-thesis.tex
%
\chapter{Evaluation}
\label{sec:evaluation}

This chapter is dedicated to the evaluation of the proposed ranking approaches. First, the oracle and baseline against which the rankers are compared are explained. Second, the experimental setup, that is conditions under which the evaluation was conducted, is set forth. Last, results are presented and discussed.

\section{Baseline and Oracle}
% Workings of oracle
In order to evaluate the implemented rankers, their outputs are compared to a perfect output generated by an oracle. The oracle is implemented as a ranker that, after having been trained on a meta data - performance data set returns correct rankings when queried for any instance of that data set. The correct ranking is implied by the performance values (highest first, as the measure used here is predictive accuracy), with ties being handled in a sequential manner, i.e. for the same performance values, the algorithm that was encountered first when constructing the ranking will be ranked before all others with the same value.

% Evaluation Measures used & computed
As rankings returned by this oracle represent the ideal solution, the predicted rankings are compared with them through the measures presented in chapter \ref{chapter:fundamentals}. These include the Kendall rank correlation coefficient, loss and best three loss. The regression based rankers pose a special case: internally, they predict a performance value for each classifier, which therefore can be compared to the actual performance values to show how well they are predicted. This is done by computing the root mean square error between the predicted and actual performance values. In summary, it is desirable for the rankers to come as close as possible to the correct ranking, which is reflected by a high rank correlation and a low loss and root mean square error.

% Baseline
Furthermore, we are interested in the question as to what degree knowing about the properties of a data set influences the quality of rankings. Therefore, a ranker that is agnostic of the meta features of a query data set, that is that will always return the same ranking for any data set, is used as a baseline. More specifically, a best algorithm strategy that iteratively determines a ranking by counting the number of data sets where an algorithm is the best choice, is implemented. That means the first algorithm in the best algorithm ranking is the classifier that performs best on most data sets, the second is the best on most data sets when only rankings excluding the first algorithm are considered, and so forth. This baseline is evaluated in the same way as the preference rankers; as it does not predict performance values, the root mean square error of predicted performance values cannot bet computed. Ideally, rankings returned by the preference and regression based rankers should then be statistically significant better than the baseline, which will be determined by calculating the Mann-Whitney U and determining whether its P-value. 

\section{Experimental Setup}
In the following paragraphs, it is briefly summarized under which conditions the experiments which led to the results discussed in the next section where observed. This includes which classifiers and data sets where chosen in the evaluation, what the specifications of the machines the evaluation was executed on where, how the evaluation was carried out, and which specific regression based and preference based implementations where used.

% Which classifiers and data sets used
The classifiers considered for a recommended ranking were the learning algorithms implemented in WEKA that are fit for classification, excluding meta and ensemble methods. The full list of the 22 classifiers can be gathered from Table \ref{tab:bestAlgorithmRanking}. They where all used in their default configurations as specified by WEKA.

datasets

% How base performance data generated
The performance data of the classifiers for the selected data sets was generated on [MACHINE DETAILS]. [TIMEOUT AND HARDWARE ALLOCATION DETAILS]. For each of the classifiers, the predictive accuracy (the percentage of correctly classified instances in the data set) was recorded on each data set by means of five times stratified Monte-Carlo cross validation with a 70/30 split in training and test data. When a timeout occurred, the predictive accuracy was set to 0.

% How rest of the evaluation carried out
All other experiments where carried out on a Windows Machine running Windows 10 Education (version 1709) with a Intel(R) Core(TM) i5-4200U CPU running at 1.60GHz (2.30GHz max) and 12 GB RAM. It is notable that especially the time-related results are to be viewed in regard to this setup. No timeouts where used in this evaluation, and the rankers where evaluated on the training data by means of leave-one-out estimation, that is for the n data sets for which performance values where recorded, they where trained with the meta data - performance examples for n - 1 data sets, and queried for the remaining data set. As n = 448 in this case, the full results are not included in the discussion of the results. They can, however, be found in the supplementary material, offering clues to how each ranker performed in the prediction of a ranking for each of the used data sets, identified by their ID on OpenML. 

% How find best version of preference and regression ranker
Furthermore, four alternatives where selected for a regression based and preference based alternative each, to get a general idea of how well the respective approach might be suited for the problem. For the regression based approach, the algorithms chosen where random forest, REPTree, M5P, and linear regression. For all of them, the WEKA implemenation \cite{hall2009weka} was used, and the algorithms where used with the standard hyperparameters set by WEKA. As the preference based ranking falls under the category of label ranking, and in the jPL-framework, which was used for the implementation of the label ranking algorithms and data set representations, only two alternatives for label ranking are implemented \cite{intelligent2017jpl}, the other two alternatives where generated by modifying the hyperparameters. The implemented alternatives are label ranking by pairwise comparison and instance based label ranking. As in early tests it became apparent that the instance based approach might hold more potential, for label ranking by pairwise comparison, only the default configuration dictated by jPL was used. For instance based label ranking, the default configuration, a configuration where the rank aggregation algorithm used by the label ranker was set to Kemeny-Young, and a configuration with Kemeny-Young rank aggregation and the number of neighbors of the base learner of the label ranker, namely k nearest neighbor (kNN), set to the square root of the number of instances in the training data set. In the following section, these eight variants are compared with the baseline and oracle mentioned in the previous section, and among each others.

\section{Results}

In the results mainly as so many data sets only taked about mean values but the full results with related to the specific data set id are included in supplementary material.

% Results of Base Algorithms and full meta features
% - scatter plot dataset -> best preferenceRanker, best regressionRanker, bestAlgorithmbaseline each Kendall

% Results include the 4 evals on all sets, how long predictions took. Meta feature calc times sperately, but may give avg of comp+predict time each time meta feature values table in appendix with mean,min,max stdev

% When removing probing

% When removing expensive meta features

% Results of optimized variants? full / no probing / no expensive

% Insights about classifiers: place best ranking, min, max rank (optional) for each classif?

\subsection{Accuracy}

\input{tables/evaluationResults}
\ref{tab:evaluationResults}

\input{tables/metaFeatureTimes}
\ref{tab:metaFeatureTimes}

\input{tables/significanceResults}
\ref{tab:significanceResults}

\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young, Baselearner KNN with n=$\sqrt{\text{number of instances}}$}

\tikzsetnextfilename{LossGraphics}
\pgfplotsset{width=\textwidth}
\begin{tikzpicture}
\begin{axis}
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=Loss,col sep=semicolon] {data/InstanceBasedLabelRankingKemenyYoung_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,red] table [x=ID,y=Loss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
\addplot [mark=none,red,samples=2,domain=0:450]{6.48};
\addplot [mark=none,black,samples=2,domain=0:450]{5.44};
\legend{IB Label KY, Best Algorithm}
\end{axis}
\end{tikzpicture}

\tikzsetnextfilename{BestThreeLossGraphics}
\pgfplotsset{width=\textwidth}
\begin{tikzpicture}
\begin{axis}
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/InstanceBasedLabelRankingKemenyYoung_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,red] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
%\addplot [mark=none,red,samples=2,domain=0:450]{6.48};
%\addplot [mark=none,black,samples=2,domain=0:450]{5.44};
\legend{IB Label KY, Best Algorithm}
\end{axis}
\end{tikzpicture}

\subsection{Build and Prediction Times}

\input{tables/times}
\ref{tab:times}
%TODO add values for pairwise comparision in this table

\subsection{Further Insights}

\input{tables/rootMeanSquareError}
\ref{tab:rootMeanSquareError}


\input{tables/bestAlgorithmRanking}
\ref{tab:bestAlgorithmRanking}







