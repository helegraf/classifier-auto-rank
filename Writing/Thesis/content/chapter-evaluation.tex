% !TEX root = ../my-thesis.tex
%
\chapter{Evaluation}
\label{sec:evaluation}

\section{Baseline and Oracle}
% Workings of oracle
In order to evaluate the implemented rankers, their outputs are compared to a perfect output generated by an oracle. The oracle is implemented as a ranker that, after having been trained on a meta data - performance data set returns correct rankings when queried for any instance of that data set. The correct ranking is implied by the performance values (highest first, as the measure used here is predictive accuracy), with ties being handled in a sequential manner, i.e. for the same performance values, the algorithm that was encountered first when constructing the ranking will be ranked before all others with the same value.

% Evaluation Measures used & computed
As a ranking returned by this oracle represents the ideal solution, 

% Baseline
Furthermore, we are interested in the question as to what degree knowing about the properties of a data set influences the quality of rankings. Therefore, a ranker that is agnostic of the meta features of a query data set, that is that will always return the same ranking for any data set, is used as a baseline. The baseline is evaluated in the same way as the preference rankers; as it does not predict performance values, the RMSE of predict performance values cannot bet computed. Ideally, rankings returned by the preference and regression based rankers should then be statistically significant better than the baseline, and come as close as possible to the correct ranking.

\section{Experimental Setup}

% Gathering data sets from OpenML

% Generating base performance data
Performance Values acquired with 5 times MCCV
why decided to use these algorithms and not others

% How find best version of preference and regression ranker

\section{Results}

In the results mainly as so many data sets only taked about mean values but the full results with related to the specific data set id are included in supplementary material.

% Results of Base Algorithms and full meta features
% - scatter plot dataset -> best preferenceRanker, best regressionRanker, bestAlgorithmbaseline each Kendall

% Results include the 4 evals on all sets, how long predictions took. Meta feature calc times sperately, but may give avg of comp+predict time each time meta feature values table in appendix with mean,min,max stdev

% When removing probing

% When removing expensive meta features

% Results of optimized variants? full / no probing / no expensive

% Insights about classifiers: place best ranking, min, max rank (optional) for each classif?

\subsection{Accuracy}

\begin{landscape}
\begin{table}[h]
\centering
	\begin{tabularx}{23,7cm}{>{\hsize=3.4\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X| >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X| >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X}
		Ranker 				& \multicolumn{4}{>{\hsize=4.0\hsize\centering\arraybackslash}X}{Kendall's Rank Correlation} & \multicolumn{4}{>{\hsize=4.0\hsize\centering\arraybackslash}X}{Loss} & \multicolumn{4}{>{\hsize=4.0\hsize\centering\arraybackslash}X}{BestThreeLoss}\\ \cline{2-13}
							 			& Min		& Max		& Mean		& Stdv 	& Min	& Max		& Mean		& Stdv 		& Min	& Max		& Mean		& Stdv	\\ \hline
		LinearRegression 				& -0.255 	& 0.896 		& 0.473	 	& 0.221 & 0 		& 86.667 	& 3.469	 	& 7.244 		& 0 		& 31.220 	& 1.267	 	& 3.011 	\\
		M5P				 				& -0.29 		& 0.870 		& 0.470	 	& 0.219 & 0 		& 82.353 	& 3.78	 	& 6.535 		& 0 		& 82.353 	& 1.508	 	& 4.757 	\\	
		RandomForest		 				& -0.281 	& 0.922 		& 0.495	 	& 0.228 & 0		& 60 		& 3.097 		& 5.745 		& 0 		& 34.634 	& 1.308	 	& 3.150 	\\	
		REPTree			 				& -0.229 	& 0.896 		& 0.412		& 0.213 & 0 		& 82.353		& 4.829	 	& 7.948 		& 0		& 34.634 	& 1.759	 	& 3.515 	\\	
		InstanceBased 					& -0.429 	& 0.870 		& 0.221	 	& 0.249 & 0 		& 82.353		& 5.401 		& 9.44 		& 0 		& 82.353 	& 3.62	 	& 8.540 	\\	
		InstanceBased\footnotemark{}		& -0.429 	& 0.887 		& 0.340	 	& 0.252 & 0 		& 98.367 	& 5.437 		& 10.323 	& 0 		& 82.353 	& 3.294	 	& 8.367 	\\	
		InstanceBased\footnotemark{}		& -0.429 	& 0.870 		& 0.335	 	& 0.249 & 0 		& 82.353 	& 5.382	 	& 9.402 		& 0 		& 82.353 	& 3.511	 	& 8.493 	\\	
		PairwiseComparison 				& -0.870 	& 0.576		& 0.014	 	& 0.234 & 0 		& 97.822 	& 9.762	 	& 13.135 	& 0 		& 82.353 	& 4.001	 	& 8.600 	\\	
		BestAlgorithm	 				& -0.437 	& 0.489 		& 0.057	 	& 0.159 & 0		& 82.353		& 6.480 		& 10.168 	& 0		& 82.353		& 3.420 		& 8.195 	\\							
	\end{tabularx}
	\label{tab:evaluationresults1}
	\caption{Evaluation results with full meta data}
\end{table}

\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young, Baselearner KNN with n=$\sqrt{\text{number of instances}}$}
\end{landscape}

\hspace{-1em}
\begin{table}[h]
	\begin{tabularx}{1.1\textwidth}{>{\hsize=1.6\hsize}X | >{\hsize=.9\hsize}X | >{\hsize=.9\hsize}X | >{\hsize=.9\hsize}X | >{\hsize=.9\hsize}X | >{\hsize=.9\hsize}X | >{\hsize=.9\hsize}X}
		Ranker 					& \multicolumn{2}{>{\hsize=2.0\hsize\centering\arraybackslash}X}{Kendall's Rank Correlation} & \multicolumn{2}{>{\hsize=2.0\hsize\centering\arraybackslash}X}{Loss} & \multicolumn{2}{>{\hsize=2.0\hsize\centering\arraybackslash}X}{BestThreeLoss}\\ \cline{2-7} 
										& Mann-Whitney U & P-Value & Mann-Whitney U	& P-Value & Mann-Whitney U & P-Value \\ \hline
		LinearRegression 				& 186381.5 & 2.702E-109 & 126861.5 & 7.694E-12 & 129028.0 & 1.327E-13 \\
		M5P				 				& 186236.0 & 6.230E-109 & 118828.5 & 1.840E-6 & 126379.5 & 1.821E-11 \\
		RandomForest		 			& 187750.5 & 9.737E-113 & 128853.5 & 1.861E-13 & 129037.0 & 1.304E-13 \\
		REPTree			 				& 181228.0 & 8.094E-97 & 111850.0 & 0.003 & 118793.0 & 1.926E-6 \\		
		InstanceBased					& 142665.5 & 8.818E-28 & 109372.0 & 0.020 & 104279.0 & 0.311 \\
		InstanceBased\footnotemark{}	& 166460.5 & 2.583E-65 & 111940.5 & 0.003 & 109211.0 & 0.022 \\
		InstanceBased\footnotemark{} 	& 166031.0 & 1.715E-64 & 110675.5 & 0.008 & 106223.0 & 0.130 \\
		PairwiseComparison				& 110610.0 & 0.008 & 120547.5 & 1.848E-7 & 107728.0 & 0.057
	\end{tabularx}
	\label{tab:evaluationresults1}
	\caption{Significance of Results Regarding Kendall Rank Correlation Coefficient}
\end{table}

\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young, Baselearner KNN with n=$\sqrt{\text{number of instances}}$}

\tikzsetnextfilename{LossGraphics}
\pgfplotsset{width=\textwidth}
\begin{tikzpicture}
\begin{axis}
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=Loss,col sep=semicolon] {data/InstanceBasedLabelRankingKemenyYoung_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,red] table [x=ID,y=Loss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
\addplot [mark=none,red,samples=2,domain=0:450]{6.48};
\addplot [mark=none,black,samples=2,domain=0:450]{5.44};
\legend{IB Label KY, Best Algorithm}
\end{axis}
\end{tikzpicture}

\tikzsetnextfilename{BestThreeLossGraphics}
\pgfplotsset{width=\textwidth}
\begin{tikzpicture}
\begin{axis}
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/InstanceBasedLabelRankingKemenyYoung_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,red] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
%\addplot [mark=none,red,samples=2,domain=0:450]{6.48};
%\addplot [mark=none,black,samples=2,domain=0:450]{5.44};
\legend{IB Label KY, Best Algorithm}
\end{axis}
\end{tikzpicture}

\subsection{Build and Prediction Times}
\hspace{-1em}
\begin{table}[h]
	\begin{tabularx}{1.1\textwidth}{>{\hsize=2.6\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X| >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X}
		Ranker 				& \multicolumn{4}{>{\hsize=4.0\hsize\centering\arraybackslash}X}{Ranker Build Time (ms)} & \multicolumn{4}{>{\hsize=4.0\hsize\centering\arraybackslash}X}{Ranker Prediction Time (ms)} \\ \cline{2-9}
										& Min		& Max		& Mean		& Stdv 	& Min	& Max		& Mean		& Stdv 	\\ \hline
		LinearRegression 				& 1454 		& 2060 		& 1580	 	& 36 	& 0 		& 86.667 	& 3.469	 	& 7.244 	\\
		M5P				 				& 3145 		& 4916 		& 3226	 	& 89 	& 0 		& 82.353 	& 3.78	 	& 6.535	\\	
		RandomForest		 				& 6048 		& 9720 		& 6236	 	& 259 	& 0		& 60 		& 3.097 		& 5.745	\\	
		REPTree			 				& 599 		& 1264 		& 629		& 38 	& 0 		& 82.353		& 4.829	 	& 7.948	\\	
		InstanceBased 					& 66 		& 550 		& 90	 		& 25 	& 0 		& 82.353		& 5.401 		& 9.44	\\	
		InstanceBased\footnotemark{}		& 66 		& 138 		& 88	 		& 12 	& 0 		& 98.367 	& 5.437 		& 10.323	\\	
		InstanceBased\footnotemark{}		& 66 		& 163		& 87	 		& 12 	& 0 		& 82.353 	& 5.382	 	& 9.402	\\	
		PairwiseComparison 				& 11784 		& 15096		& 12623	 	& 348 	& 0 		& 97.822 	& 9.762	 	& 13.135	\\	
		BestAlgorithm	 				& 193	 	& 321		& 221	 	& 23	 	& 0		& 82.353		& 6.480 		& 10.168	\\							
	\end{tabularx}
	\label{tab:evaluationresults1}
	\caption{Build and Prediction Times of the Rankers rounded to Full Milliseconds (Full Meta Data)}
\end{table}

\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young, Baselearner KNN with n=$\sqrt{\text{number of instances}}$}

\subsection{Further Insights}

\begin{table}[h]
	\begin{tabularx}{\textwidth}{>{\hsize=1.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X | >{\hsize=.8\hsize}X}
		Ranker 				& \multicolumn{4}{>{\hsize=4.0\hsize\centering\arraybackslash}X}{Root Mean Square Error} \\ \cline{2-5}
										& Min		& Max		& Mean		& Stdv 	\\ \hline
		LinearRegression 				& 1454 		& 2060 		& 1580	 	& 36 	\\
		M5P				 				& 3145 		& 4916 		& 3226	 	& 89 	\\	
		RandomForest		 				& 6048 		& 9720 		& 6236	 	& 259 	\\	
		REPTree			 				& 599 		& 1264 		& 629		& 38 	\\								
	\end{tabularx}
	\label{tab:evaluationresults1}
	\caption{Root Mean Square Error of Predicted Performance Values for the Regression-Based Rankers (full meta data)}
\end{table}

% TODO replace by large data set??
\begin{table}[h]
\centering
	\begin{tabularx}{\textwidth}{X | X}
		%\hline
		Classifier				&	Number of Data Sets Placed First \\	\hline
		Logistic					&	28								\\	\hline	
		Naive Bayes				&	29								\\	\hline	
		IBk						&	23								\\	\hline	
		KStar					&	24								\\	\hline	
		LMT						&	17								\\	\hline	
		VotedPerceptron			&	16								\\	\hline	
		ZeroR					&	14								\\	\hline	
		J48						&	13								\\	\hline	
		NaiveBayesMultinomial	&	6								\\	\hline	
		RandomTree				&	2								\\	\hline	
		SimpleLogistic			&	1								\\	\hline
		DecisionStump			&	1								\\	\hline
		MultilayerPerceptron		&	1								\\	\hline
		RandomForest				&	1								\\	\hline
		DecisionTable			&	1								\\	\hline
		PART						&	1								\\	\hline
		SGD						&	1								\\	\hline
		BayesNet					&	1								\\	\hline
		REPTree					&	1								\\	\hline
		JRip						&	1								\\	\hline
		SMO						&	313								\\	\hline
		OneR						&	448								\\	\hline																	
	\end{tabularx}
	\label{tab:table1}
	\caption{The ranking of Classifiers returned by the Best Algorithm Baseline}
\end{table}







