% !TEX root = ../my-thesis.tex
%
\chapter{Evaluation}
\label{sec:evaluation}

This chapter is dedicated to the evaluation of the proposed ranking approaches. First, the oracle and baseline against which the rankers are compared are explained. Second, the experimental setup, that is conditions under which the evaluation was conducted, is set forth. Last, results are presented and discussed.

\section{Baseline and Oracle}
% Workings of oracle
In order to evaluate the implemented rankers, their outputs are compared to a perfect output generated by an oracle. The oracle is implemented as a ranker that, after having been trained on a meta data - performance data set returns correct rankings when queried for any instance of that data set. The correct ranking is implied by the performance values (highest first, as the measure used here is predictive accuracy), with ties being handled in a sequential manner, i.e. for the same performance values, the algorithm that was encountered first when constructing the ranking will be ranked before all others with the same value.

% Evaluation Measures used & computed
As rankings returned by this oracle represent the ideal solution, the predicted rankings are compared with them through the measures presented in chapter \ref{chapter:fundamentals}. These include the Kendall rank correlation coefficient, loss and best three loss. For all three measures, it is computed if any advantages or disadvantages one solutions has is significant, by means of the Mann-Whitney U. The regression based rankers pose a special case: internally, they predict a performance value for each classifier, which therefore can be compared to the actual performance values to show how well they are predicted. This is done by computing the root mean square error between the predicted and actual performance values. In summary, it is desirable for the rankers to come as close as possible to the correct ranking, which is reflected by a high rank correlation and a low loss and root mean square error.

% Baseline
Furthermore, we are interested in the question as to what degree knowing about the properties of a data set influences the quality of rankings. Therefore, a ranker that is agnostic of the meta features of a query data set, that is that will always return the same ranking for any data set, is used as a baseline. More specifically, a best algorithm strategy that iteratively determines a ranking by counting the number of data sets where an algorithm is the best choice, is implemented. That means the first algorithm in the best algorithm ranking is the classifier that performs best on most data sets, the second is the best on most data sets when only rankings excluding the first algorithm are considered, and so forth. This baseline is evaluated in the same way as the preference rankers; as it does not predict performance values, the root mean square error of predicted performance values cannot bet computed. Ideally, rankings returned by the preference and regression based rankers should then be statistically significant better than the baseline, which will be determined by calculating the Mann-Whitney U and determining whether its P-value. 

\section{Experimental Setup}
In the following paragraphs, it is briefly summarized under which conditions the experiments which led to the results discussed in the next section where observed. This includes which classifiers, data sets and meta features where chosen in the evaluation, what the specifications of the machines the evaluation was executed on where, how the evaluation was carried out, and which specific regression based and preference based implementations where used.

% Which classifiers 
The classifiers considered in the rankings were the learning algorithms implemented in WEKA that are fit for classification, excluding meta and ensemble methods. The full list of the 22 classifiers can be gathered from Table \ref{tab:bestAlgorithmRanking}. They where all used in their default configurations as specified by WEKA. 

% Which data sets
Data sets for the analysis where gathered from OpenML. From all data sets considered as 'active' on OpenML, which yields 2050 data sets that have been approved and with no severe issues found for them so far \cite{openMLGuide}, all data sets that comply with the constraints of the learning problem at hand where selected. Only data sets with a defined, nominal target feature that are in the .ARFF format and where not specifically designed for streaming analysis\footnotemark{} where considered, which resulted in a reduced selection of 812 instances. Since this amount of data sets hindered the evaluation considerably, large data sets with more than 1000 instances or 100 features where removed as well, leading to a final selection of 448 data sets. Even though evaluation was only carried out on the shorter list, both the list of 812 and 448 data sets are included in the supplementary material for completeness.

\footnotetext{Data sets that contained the substring 'BNG' for Bayesian Network Generator, which contain data artificially generated by a Bayesian Network\cite{van2014algorithm} for the sake of data stream analysis where not included in the evaluation.}

% Which meta features
As for the meta features, all meta features discussed in chapter \ref{sec:approach} where used. Initially, it was intended to download the computed meta features from OpenML, but since missing values for meta features ranged from roughly 25\% up to 89\% (except for a few simple meta features which were included for all data sets) it was decided to compute them anew. Meta features where calculated with the help of an implementation provided by OpenML \cite{openMLEvaluationEngine}.Two data sets, one including the downloaded meta features (for all 812 data sets) and one including the computed meta features (for the reduced set of 448 data sets) are thus included in the supplementary material. 

% How base performance data generated
The performance data of the classifiers for the selected data sets was generated on [MACHINE DETAILS]. [TIMEOUT AND HARDWARE ALLOCATION DETAILS]. For each of the classifiers, the predictive accuracy (the percentage of correctly classified instances in the data set) was recorded on each data set by means of five times stratified Monte-Carlo cross validation with a 70/30 split in training and test data. When a timeout occurred, the predictive accuracy was set to 0.

% How rest of the evaluation carried out
All other experiments where carried out on a Windows Machine running Windows 10 Education (version 1709) with a Intel(R) Core(TM) i5-4200U CPU running at 1.60GHz (2.30GHz max) and 12 GB RAM. It is notable that especially the time-related results are to be viewed in regard to this setup. No timeouts where used in this evaluation, and the rankers where evaluated on the training data by means of leave-one-out estimation, that is for the n data sets for which performance values where recorded, they where trained with the meta data - performance examples for n - 1 data sets, and queried for the remaining data set. As n = 448 in this case, the detailed values for each data set are not included in the discussion of the results. They can, however, be found in the supplementary material, offering clues to how each ranker performed in the prediction of a ranking for each of the used data sets, identified by their ID on OpenML. 

% How find best version of preference and regression ranker
Furthermore, four alternatives where selected for a regression based and preference based alternative each, to get a general idea of how well the respective approach might be suited for the problem. For the regression based approach, the algorithms chosen where random forest, REPTree, M5P, and linear regression. For all of them, the WEKA implemenation \cite{hall2009weka} was used, and the algorithms where used with the standard hyperparameters set by WEKA. As the preference based ranking falls under the category of label ranking, and in the jPL-framework, which was used for the implementation of the label ranking algorithms and data set representations, only two alternatives for label ranking are implemented \cite{intelligent2017jpl}, the other two alternatives where generated by modifying the hyperparameters. The implemented alternatives are label ranking by pairwise comparison and instance based label ranking. As in early tests it became apparent that the instance based approach might hold more potential, for label ranking by pairwise comparison, only the default configuration dictated by jPL was used. For instance based label ranking, the default configuration, a configuration where the rank aggregation algorithm used by the label ranker was set to Kemeny-Young, and a configuration with Kemeny-Young rank aggregation and the number of neighbors of the base learner of the label ranker, namely k nearest neighbor (kNN), set to the square root of the number of instances in the training data set. In the following section, these eight variants are compared with the baseline and oracle mentioned in the previous section, and among each others.

\section{Results}

In the following subsections, results of the evaluation are discussed in detail. First, the quality of the predictions of the rankers is examined. Then, the times required for building the rankers and for predictions made as well as for the calculation of the meta features is reviewed. Last, additional insights gathered during the evaluation are pointed out.

\subsection{Accuracy}

- on the full meta data set, all approaches except for instance based with kemeny young and pairwise comparison 

\input{tables/evaluationResults}
\ref{tab:evaluationResults}

\input{tables/significanceResults}
\ref{tab:significanceResults}

\tikzsetnextfilename{LossGraphics}
\begin{figure}
\pgfplotsset{width=.5\textwidth}
\begin{tikzpicture}
\begin{axis}[ylabel={BestThreeLoss}, xlabel={Data Set}]
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,uniaccentblue] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/RandomForestRanker_metaData_small_allPerformanceValues.csv};
\addplot [very thick,mark=none,black,samples=2,domain=0:450]{2.440};
\addplot [very thick,mark=none,uniaccentblue,samples=2,domain=0:450]{1.308};
\legend{BestAlgorithm, RandomForest, BestAlgorithm (mean), RandomForest (mean)}
\end{axis}
\end{tikzpicture}%
~%
%
%\begin{tikzpicture}
%\begin{axis}[ymin=-5, ymax=25,ylabel={BestThreeLoss}, xlabel={Data Set}]
%\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
%\addplot [mark=*,only marks,mark size=1pt,uniaccentblue] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/RandomForestRanker_metaData_small_allPerformanceValues.csv};
%\addplot [very thick,mark=none,black,samples=2,domain=0:450]{2.440};
%\addplot [very thick,mark=none,uniaccentblue,samples=2,domain=0:450]{1.308};
%\legend{BestAlgorithm, RandomForest, BestAlgorithm (mean), RandomForest (mean)}
%\end{axis}
%\end{tikzpicture}
\caption{The Best Three Loss for the Best Algorithm Baseline and the Random Forest Ranker (Full Meta Data)}
\label{fig:lossGraphics}
\end{figure}


\subsection{Computation Times}

Because the ranking itself is implemented as a means of speeding up the process of algorithm selection, the time it takes to rank is not negligible. The full results of how long it takes to build the respective ranking models and how much time they need for predictions can be taken from Table \ref{tab:times}. Results are rounded to the full millisecond, as due to the times being measured with a stop watch in the code, more accurate measurements are not possible anyways. The times for building the regression rankers are not surprising; the regression models take far more time to be built than most of the label ranking methods, most likely due to the fact the regression based rankers have to train 22 regression models in order to be trained themselves. However, it is notable that while the ranking by Pairwise Comparison was not among the best solutions concerning the accuracy of the results, it is the ranker with the highest mean build time for full meta data, and second highest for a reduced meta data set, whereas in other cases the high quality of results is correlated with a high time invested in the building of the ranker likewise. The Random Forest ranker, for example, is the ranker with the highest build time, and also with the rankings of the generally highest quality. But the most important fact to be taken away from Table \ref{tab:times} is that the prediction times for all rankers are short enough to be negligible, with the mean prediction time never being higher than five milliseconds and the maximum never exceeding 50 milliseconds. Furthermore, while the build times are higher, these are not as important due to the fact that each model only needs to be trained once. Although in a real-world scenario, calculation times for meta features also have to be taken into account when ranking, which therefore will be discussed in the following paragraph.

\input{tables/times}

Table \ref{tab:metaFeatureTimes} shows the times required For the full meta features, on average approximately 135 milliseconds would be added to the prediction times 3 milliseconds whithout probing

\input{tables/metaFeatureTimes}

\subsection{Further Insights}

Further insights to be noted are the accuracy of predictions of performance values for classifiers by the regression models, and the static ranking constructed by the best algorithm baseline. The accuracy of the predictions achieved by the regression based rankers are depicted in Table \ref{tab:rootMeanSquareError}, for full and reduced meta data respectively. The first thing that attracts attention in this table is the absurdly high values for Linear Regression and M5P, however, these are all caused by the very high maximum value on one data set with id 685. If these extreme maxima remain out of consideration for Linear Regression and M5P (the other algorithms do not have this problem) more reasonable values are obtained. For M5P, on the full meta data the new maximum is 311.526, mean 8.127 and standard deviation 18.407; on the reduced meta data it is a maximum of 474.692, mean 15.444 and standard deviation 27.433. For Linear Regression, on the full meta data the new maximum is 142.996, mean 9.281 and standard deviation 8.438; on the reduced meta data it is a maximum of 131.456, mean 14.351 and standard deviation 10.299. However, these values are still worse than the ones obtained for the Random Forest and REPTree variants. It comes as little surprise that the Random Forest Ranker also delivers the most accurate results for performance estimates, but as these best values are more than 6\% off on average for the full meta data and even more than 10\% off on average for the reduced meta data, it is questionable whether these prediction can be useful.

\input{tables/rootMeanSquareError}

The second interesting thing to consider is the ranking which is created by the best algorithm baseline. This ranking constitutes meta-knowledge about the learning process itself in that it shows which algorithms are generally a good choice for many data sets. This ranking is depicted in Table \ref{tab:bestAlgorithmRanking}. The table is to be read top to bottom; of the 22 algorithms, the algorithm in the ith row is placed first on most data sets when compared to the next 22-i algorithms. The associated number represents the number of data sets on which the algorithm was placed first. It is, however, not a particularly surprising result, as for example the placement of Random Forest as the first algorithm was to be expected. Random Forest is a learning algorithm that is known to often deliver good performances, which was proven again in the results discussed previously - out of the reviewed ranking approaches, Random Forest was the best choice.

\input{tables/bestAlgorithmRanking}








