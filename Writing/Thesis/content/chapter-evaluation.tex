% !TEX root = ../my-thesis.tex
%
\chapter{Evaluation}
\label{sec:evaluation}

This chapter is dedicated to the evaluation of the proposed ranking approaches. First, the oracle and baseline against which the rankers are compared are explained. Second, the experimental setup, that is conditions under which the evaluation was conducted, is set forth. Last, results are presented and discussed.

\section{Baseline and Oracle}
% Workings of oracle
In order to evaluate the implemented rankers, their outputs are compared to a perfect output generated by an oracle. The oracle is implemented as a ranker that, after having been trained on a meta data - performance data set returns correct rankings when queried for any instance of that data set. The correct ranking is implied by the performance values (highest first, as the measure used here is predictive accuracy), with ties being handled in a sequential manner, i.e. for the same performance values, the algorithm that was encountered first when constructing the ranking will be ranked before all others with the same value.

% Evaluation Measures used & computed
As rankings returned by this oracle represent the ideal solution, the predicted rankings are compared with them through the measures presented in chapter \ref{chapter:fundamentals}. These include the Kendall rank correlation coefficient, loss and best three loss. The regression based rankers pose a special case: internally, they predict a performance value for each classifier, which therefore can be compared to the actual performance values to show how well they are predicted. This is done by computing the root mean square error between the predicted and actual performance values. In summary, it is desirable for the rankers to come as close as possible to the correct ranking, which is reflected by a high rank correlation and a low loss and root mean square error.

% Baseline
Furthermore, we are interested in the question as to what degree knowing about the properties of a data set influences the quality of rankings. Therefore, a ranker that is agnostic of the meta features of a query data set, that is that will always return the same ranking for any data set, is used as a baseline. More specifically, a best algorithm strategy that iteratively determines a ranking by counting the number of data sets where an algorithm is the best choice, is implemented. That means the first algorithm in the best algorithm ranking is the classifier that performs best on most data sets, the second is the best on most data sets when only rankings excluding the first algorithm are considered, and so forth. This baseline is evaluated in the same way as the preference rankers; as it does not predict performance values, the root mean square error of predicted performance values cannot bet computed. Ideally, rankings returned by the preference and regression based rankers should then be statistically significant better than the baseline, which will be determined by calculating the Mann-Whitney U and determining whether its P-value. 

\section{Experimental Setup}

% Gathering data sets from OpenML
how many data sets from OpenML

% Generating base performance data
Performance Values acquired with 5 times MCCV
which (hyper)parameters where used
which timeouts used
why decided to use these algorithms and not others

% How find best version of preference and regression ranker

\section{Results}

In the results mainly as so many data sets only taked about mean values but the full results with related to the specific data set id are included in supplementary material.

% Results of Base Algorithms and full meta features
% - scatter plot dataset -> best preferenceRanker, best regressionRanker, bestAlgorithmbaseline each Kendall

% Results include the 4 evals on all sets, how long predictions took. Meta feature calc times sperately, but may give avg of comp+predict time each time meta feature values table in appendix with mean,min,max stdev

% When removing probing

% When removing expensive meta features

% Results of optimized variants? full / no probing / no expensive

% Insights about classifiers: place best ranking, min, max rank (optional) for each classif?

\subsection{Accuracy}

\input{tables/evaluationResults}
\ref{tab:evaluationResults}

\input{tables/metaFeatureTimes}
\ref{tab:metaFeatureTimes}

\input{tables/significanceResults}
\ref{tab:significanceResults}

\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young}
\stepcounter{footnote}\footnotetext{Rankaggregation: Kemeny-Young, Baselearner KNN with n=$\sqrt{\text{number of instances}}$}

\tikzsetnextfilename{LossGraphics}
\pgfplotsset{width=\textwidth}
\begin{tikzpicture}
\begin{axis}
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=Loss,col sep=semicolon] {data/InstanceBasedLabelRankingKemenyYoung_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,red] table [x=ID,y=Loss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
\addplot [mark=none,red,samples=2,domain=0:450]{6.48};
\addplot [mark=none,black,samples=2,domain=0:450]{5.44};
\legend{IB Label KY, Best Algorithm}
\end{axis}
\end{tikzpicture}

\tikzsetnextfilename{BestThreeLossGraphics}
\pgfplotsset{width=\textwidth}
\begin{tikzpicture}
\begin{axis}
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/InstanceBasedLabelRankingKemenyYoung_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,red] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
%\addplot [mark=none,red,samples=2,domain=0:450]{6.48};
%\addplot [mark=none,black,samples=2,domain=0:450]{5.44};
\legend{IB Label KY, Best Algorithm}
\end{axis}
\end{tikzpicture}

\subsection{Build and Prediction Times}

\input{tables/times}
\ref{tab:times}
%TODO add values for pairwise comparision in this table

\subsection{Further Insights}

\input{tables/rootMeanSquareError}
\ref{tab:rootMeanSquareError}


\input{tables/bestAlgorithmRanking}
\ref{tab:bestAlgorithmRanking}







