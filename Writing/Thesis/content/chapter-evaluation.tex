\chapter{Evaluation}
\label{sec:evaluation}

This chapter is dedicated to the evaluation of the proposed ranking approaches. First, the oracle and baseline against which the rankers are compared are explained. Second, the experimental setup, that is conditions under which the evaluation was conducted, is set forth. Last, results are presented and discussed.

\section{Baseline and Oracle}
% Workings of oracle
In order to evaluate the implemented rankers, their outputs are compared to a perfect output generated by an oracle. The oracle is implemented as a ranker that, after having been trained on a meta data-performance data set, returns correct rankings when queried for any instance of that data set. The correct ranking is implied by the performance values (highest first, as the measure used here is predictive accuracy), with ties being handled in a sequential manner, i.e. for the same performance values, the algorithm that was encountered first when constructing the ranking will be ranked before all others with the same value.

% Evaluation Measures used & computed
As rankings returned by this oracle represent the ideal solution, the predicted rankings are compared with them through the measures presented in chapter \ref{sec:fundamentals}. These include the Kendall rank correlation coefficient, performance loss and B3L regarding classifier performance. The measure chosen for the performance values here is the \textit{predictive accuracy} of a classifier, meaning the number of correctly classified instances in a data set. For all three measures, it is computed whether any difference in performance is significant, by means of the p-value for the Mann-Whitney U. The regression-based rankers pose a special case: internally, they predict a performance value for each classifier, which therefore can be compared to the actual performance values to show how well they are predicted. This is done by computing the root mean square error between the predicted and actual performance values. In summary, it is desirable for the rankers to come as close as possible to the correct ranking, which is reflected by a high rank correlation and a low loss and root mean square error.

% Baseline
Furthermore, we are interested in the question as to what degree knowing about the properties of a data set influences the quality of rankings. Therefore, a ranker that is agnostic to the meta features of a query data set, meaning that it will always return the same ranking for any data set, is used as a baseline. More specifically, a best algorithm strategy that iteratively determines a ranking by counting the number of data sets where an algorithm is the best choice, is implemented. That means the first algorithm in the best algorithm ranking is the classifier that performs best on most data sets, the second is the best on most data sets when only rankings excluding the first algorithm are considered, and so forth. This baseline is evaluated in the same way as the preference rankers; as it does not predict performance values, the root mean square error of predicted performance values cannot be computed. Ideally, rankings returned by the preference and regression based rankers should then be statistically significantly better than the static baseline, as this would indicate a connection of certain meta features and a performance-induced ranking of classifiers for a data set. 

\section{Experimental Setup}
In the following paragraphs, it is briefly summarized under which conditions the experiments which led to the results discussed in the next section were observed. This includes which classifiers and data sets were chosen, and the specifications of the machines the evaluation was executed on. It is described how the evaluation was carried out, and which specific regression-based and preference-based implementations were used.

% Which classifiers 
The classifiers considered in the rankings were the learning algorithms implemented in WEKA that are fit for classification, excluding meta and ensemble methods. The full list of the 22 classifiers can be gathered from Table \ref{tab:bestAlgorithmRanking}. They were all used in their default configurations as specified by WEKA. 

% Which data sets
Data sets for the analysis were gathered from OpenML. From all data sets considered as `active' on OpenML, which yields 2050 data sets that have been approved with no severe issues found for them so far \cite{openMLGuide}, all data sets that comply with the constraints of the learning problem at hand were selected. Only data sets in the .ARFF format with a defined, nominal target feature which were not specifically designed for streaming analysis\footnotemark{} were considered, which resulted in a reduced selection of 812 instances. Since this amount of data sets hindered the evaluation considerably, large data sets with more than 1000 instances or 100 features were removed as well, leading to a final selection of 448 data sets. Even though evaluation was only carried out on the shorter list, both the list of 812 and 448 data sets are included in the supplementary material for completeness.

\footnotetext{Data sets that contained the substring `BNG' in their name for Bayesian Network Generator, which contain data artificially generated by a Bayesian Network \cite{van2014algorithm} for the sake of data stream analysis were not included in the evaluation.}

% How base performance data generated
The performance data of the classifiers for the selected data sets was generated on a linux cluster with nodes consisting of two Intel(R) Xeon(R) E5-2670 processors running at 2.6GHz, 64GB RAM. For the computation of a single performance value of a classifier on a data set, two CPU cores and 16GB RAM were used, with a timeout of eight hours. Up to 150 of such processes were executed in parallel. For each of the classifiers, the predictive accuracy (the percentage of correctly classified instances in the data set) was recorded on each data set by means of five times stratified Monte-Carlo cross validation with a 70/30 split in training and test data. The stratified split was generated with the help of JAICore, `A Java library for infrastructural classes and basic algorithms in the area reasoning, search, and machine learning' \cite{jaicore}. When a timeout occurred or the evaluation failed otherwise, the predictive accuracy was set to zero.

% How rest of the evaluation carried out
All other experiments were carried out on a Windows Machine running Windows 10 Education (version 1709) with a Intel(R) Core(TM) i5-4200U CPU running at 1.60GHz (2.30GHz max) with 12 GB RAM. It is notable that especially the time-related results are to be viewed in regard to this setup. No timeouts were used in this evaluation, and the rankers were evaluated on the training data by means of leave-one-out estimation, that is for the n data sets for which performance values were recorded, they were trained with the meta data - performance examples for n - 1 data sets, and queried for the left-out data set. As n = 448 in this case, the detailed values for each data set are not included in the discussion of the results. They can, however, be found in the supplementary material, offering clues to how each ranker performed in the prediction of a ranking for each of the used data sets, identified by their ID on OpenML. Furthermore, the evaluation was carried out twice for all measures: once with the full meta data used, and one time using only the meta features not generated by landmarkers, that is the ones that require probing to be carried out for a new data set.

% Implementation of measures
For the computation of the Mann-Whitney U and Kendall rank correlation coefficient, Apache Commons implementations were used \cite{apache}. Regarding the Mann-Whitney U, the strategy for ties was set to averaging, and the NaNs were set to be removed before the analysis was started.

% How find best version of preference and regression ranker
Furthermore, four alternatives were selected for a regression-based and preference-based alternative each, to get a general idea of how well the respective approach might be suited for the problem. For the regression-based approach, the algorithms chosen were random forest, REPTree, M5P, and linear regression. For all of them, the WEKA implemenation \cite{hall2009weka} was used, and the algorithms where used with the standard hyperparameters set by WEKA. As the preference-based ranking falls under the category of label ranking, and in the jPL-framework, which was used for the implementation of the label ranking algorithms and data set representations, only two alternatives for label ranking are implemented \cite{intelligent2017jpl}, the other two alternatives were generated by modifying the hyperparameters. The implemented alternatives are label ranking by pairwise comparison and instance-based label ranking. As in early tests it became apparent that the instance-based approach might hold more potential, for label ranking by pairwise comparison, only the default configuration dictated by jPL was used. For instance-based label ranking, three different configurations were evaluated, the default configurations and two custom ones. For the first customization, the rank aggregation algorithm used by the label ranker was set to Kemeny-Young. For the second customization, additionally, the number of neighbors of the base learner of the label ranker, namely k nearest neighbor (kNN), was set to the square root of the number of instances in the training data set. All of the described rankers are included in an overview in Table \ref{tab:methods}. In the following section, these eight variants are compared with the baseline and oracle mentioned in the previous section, and among each others. They are referenced by the names in Table \ref{tab:methods} to avoid confusion between the ranker and the learning algorithm it is based on.

\begin{table}
\resizebox{\textwidth}{!}{%
\begin{tabularx}{1.25\textwidth}{>{\hsize=.4\hsize}X | >{\hsize=1.6\hsize}X}
	Ranker				& Configuration \\ \hline\hline
	LinearRegression		& Linear regression with S:0, R:1.0E-8, num-decimal-places:4 \\ \hline
	M5P					& M5 model trees with M:4 \\ \hline
	RandomForest			& Random forest with P:100, I:100, num-slots:1, K:0, M:1.0, V:0.001, S:1 \\ \hline
	REPTree				& REPT tree with M:2, V:0.001, N:3, S:1, L:1, I: 0.0 \\ \hline \hline	
	InstanceBased		& Instance-based label ranking with rankaggregation: Placket-Luce (norm tolerance: 1E-9, max iterations: 1000, log likelihood tolerance: 1E-5), baselearner: kNN (k: 10) \\ \hline
	InstanceBased		& Instance-based label ranking with rankaggregation: Kemeny-Young, baselearner: kNN (k: 10) \\ \hline
	InstanceBased		& Instance-based label ranking with rankaggregation: Kemeny-Young, baselearner: kNN (k:$\sqrt{\text{number of instances}}$) \\ \hline
	PairwiseComparison 	& Label ranking by pairwise comparison with baselearner: logistic regression (learning rate: 0.001), gradient step: adam (beta1: 0.99, beta2: 0.999, epsilon: 10E-8) \\
\end{tabularx}
}
\caption{An overview over the different base algorithms used in the evaluated rankers.}
\label{tab:methods}
\end{table}

\section{Results}

In the following subsections, results of the evaluation are discussed in detail. First, the quality of the predictions of the rankers is examined. Then, the times required for building the rankers and for predictions made as well as for the calculation of the meta features is reviewed. At last, additional insights gathered during the evaluation are pointed out.

\subsection{Accuracy}

Table \ref{tab:evaluationResults} shows the results of the evaluation regarding the accuracy of returned rankings, with the best mean values and significant differences emphasized as described in the caption of the table. When looking at these results, it must first be noted that positive findings regarding the Kendall rank correlation can be reported. For the full set of meta data, six out of the eight implemented alternatives outperform the baseline, all of them significantly, as can be learned from Table \ref{tab:significanceResults}, which shows the results of the significance tests according to the p-value for the Mann-Whitney U. Of all rankers, RandomForest has the highest rank correlation with a value of 0.495. In contrast, the two approaches that did not overtake the baseline, the InstanceBased ranker in the default configuration and the PairwiseComparison ranker, show a significant disadvantage compared to the baseline, and thus also compared to the other ranking approaches. Especially the latter approach has a very low rank correlation, that with a mean value of 0.014 shows almost no correlation with the correct ranking. 

Regarding performance loss, meaning much worse the performance of the first recommended algorithm is in comparison to the actual best algorithm on the data set, as defined in Chapter \ref{sec:fundamentals}, only the LinearRegression and RandomForest ranker show a significant advantage, with all label ranking approaches except for the InstanceBased ranker with Kemeny-Young rank aggregation having a significant disadvantage in contrast. Here, an advantage of the regression-based approach in general is indicated, although the best value with a loss of 3.097 (achieved by the RandomForest ranker) is still fairly high, depending on the data set one considers. 

Moving on to B3L, the picture largely remains the same, except that surprisingly, the REPTree ranker now also has a significant advantage over the baseline. The lowest value here is achieved by the inearRegression ranker with a mean B3L of 1.267, in comparison to 2.440 for the best algorithm baseline. These values are illustrated in Figure \ref{fig:lossGraphics}, with a scatter plot for individual B3L values on the data sets. In this figure, we can observe that, for a large number of data sets, the B3L is actually 0, namely 206 data sets for the BestAlgorithm ranker and 248 for the RandomForest ranker, which corresponds to 46\% and 55\% of the 448 data sets used in the evaluation. The main portion of the remaining data points is situated between 0.25 and 8 for the RandomForestRanker, and 0.5 and 16 for the BestAlgorithm ranker, with few exceptions. 

As for the rank correlation most of the rankers had a significant advantage over the baseline, it is interesting to compare them among each other. From Table \ref{tab:evaluationResults} it is clear that the regression-based rankers are superior to the preference-based rankers, and this advantage is significant, as the weakest of the regression rankers, the REPTree ranker, is still significantly better than the strongest label ranking approach, the InstanceBased ranker with Kemeny-Young rank aggregation, as indicated by a p-value of 4.305E-6. Out of the four regression rankers, three approaches are furthermore significantly better than the REPTree ranker. 

Compared to the evaluation on the full set of meta data, the results on the reduced set without probing are less distinctive. For the Kendall Rank correlation, the LinearRegression, M5P and RandomForest rankers are significantly better than the baseline, and for label ranking, both of the InstanceBased approaches with non-default parameters as well, while the other label ranking approaches are significantly worse than the baseline. In total, the RandomForest ranker is still the best alternative with a correlation of 0.439, slightly worse than the correlation achieved on the full meta data. For both loss measures however, while the disadvantages of the label ranking approaches remain, none of the eight alternatives achieves a significantly better loss or B3L than the baseline anymore.

Again, the different approaches can be compared among each other. With reduced meta data, out of the alternatives that pose a significant advantage compared to the baseline, only the RandomForest ranker is significantly better than all other approaches, the others are not significantly different. With loss and B3L, no such statement can be made.

Further interesting points can be observed. First, in Table \ref{tab:evaluationResults} one can see that for the loss and B3L, often, rankers have a similar values. Second, the performance of the preference-based rankers does not differ very much, whether full meta data or reduced meta data was used in the evaluation. While, for example, Kendall's Rank correlation for the LinearRegression ranker is 0.473 for full meta data, it is noticeably lower for reduced meta data with a value of 0.350. On the other hand, the correlation coefficient for the InstanceBased rankers in all configurations does not change at all when removing meta features that involve probing.

Summarizing the observed results, in general, a regression-based approach shows an indication to be stronger than a preference-based approach. Specifically, using random forest as a regression model is the best solution observed in the evaluation, and label ranking by pairwise comparison the weakest, as it falls behind the other ranking approaches and the baseline often. A surprising result is that the label ranking approaches achieve very similar results with or without probing.

\input{tables/evaluationResults}

\input{tables/significanceResults}

\tikzsetnextfilename{LossGraphics}
\begin{figure}
%\pgfplotsset{width=\textwidth}
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[
	title=regular scale,
	height=\textwidth,
	width=\textwidth,
	ylabel={B3L}, 
	xlabel={Data Set},
]
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,uniaccentblue] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/RandomForestRanker_metaData_small_allPerformanceValues.csv};
\addplot [very thick,mark=none,black,samples=2,domain=0:450]{2.440};
\addplot [very thick,mark=none,uniaccentblue,samples=2,domain=0:450]{1.308};
\legend{BestAlgorithm, RandomForest, BestAlgorithm (mean), RandomForest (mean)}
\end{axis}
\end{tikzpicture}%
~%
%
\begin{tikzpicture}
\begin{axis}[
	title=log scale (base 2),
	height=\textwidth,
	width=\textwidth,
	ylabel={B3L}, 
	xlabel={Data Set},
	ymode=log,
	log basis y={2}
]
\addplot [mark=*,only marks,mark size=1pt] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/BestAlgorithmRanker_metaData_small_allPerformanceValues.csv};
\addplot [mark=*,only marks,mark size=1pt,uniaccentblue] table [x=ID,y=BestThreeLoss,col sep=semicolon] {data/RandomForestRanker_metaData_small_allPerformanceValues.csv};
\addplot [very thick,mark=none,black,samples=2,domain=0:450]{2.440};
\addplot [very thick,mark=none,uniaccentblue,samples=2,domain=0:450]{1.308};
\legend{BestAlgorithm, RandomForest, BestAlgorithm (mean), RandomForest (mean)}
\end{axis}
\end{tikzpicture}
}
\caption{The B3L for the baseline compared to the random forest ranker (full meta data).}
\label{fig:lossGraphics}
\end{figure}


\subsection{Computation Times}

Because the ranking itself is implemented as a means of speeding up the process of algorithm selection, the time it takes to rank is not negligible. The full results of how long it takes to build the respective ranking models and how much time they need for predictions can be taken from Table \ref{tab:times}. Results are rounded to the full millisecond, as due to the times being measured with a stop watch in the code, more accurate measurements are not possible. The times for building the regression rankers are not surprising; the regression models take far more time to be built than most of the label ranking methods, most likely due to the fact the regression based rankers have to train 22 regression models in order to be trained themselves. However, it is notable that while the PairwiseComparison ranker was not among the best solutions concerning the accuracy of the results, it is the ranker with the highest mean build time for full meta data, and second highest for a reduced meta data set, whereas in other cases the high quality of results is correlated with a high time invested in the building of the ranker. The RandomForest ranker, for example, is the ranker with the highest build time (apart from the PairwiseComparison ranker), and also with the rankings of the generally highest quality. But the most important fact to be taken away from Table \ref{tab:times} is that the prediction times for all rankers are short enough to be negligible, with the mean prediction time never being higher than five milliseconds and the maximum never exceeding 50 milliseconds. Furthermore, while the build times are higher, these are not as important due to the fact that each model only needs to be trained once. Although in a real-world scenario, calculation times for meta features also have to be taken into account when ranking, which therefore will be discussed in the following paragraph.

\input{tables/times}

Table \ref{tab:metaFeatureTimes} shows the times required for the computation of groups of meta features as discussed in Chapter \ref{sec:approach}. As can be taken from the table, the computation of the full meta features takes approximately 135 milliseconds on average, while the simple meta features without probing add approximately 3 milliseconds to the prediction times on average, which is already a difference of factor 45, which is quite large especially considering the fact that only light data sets, restricted in the number of features and instances, were used. To emphasize this difference, the meta features were calculated for the popular MNIST data set \cite{mnist}, which contains pictures of handwritten digits in a 28 by 28 pixel resolution, resulting in 784 features, plus the target feature. The data set contains 70.000 different pictures. For the computation of the full set of meta features, 4 hours and 51 minutes were needed, out of which only 24 seconds were needed for the reduced set of meta features, a difference of factor 735. Regarding these computation times it is also notable that among the probing meta feature groups, some are considerably more expensive than others, so that in the future it might be sensible to consider only light probing with probing meta feature groups that are not as expensive.

\input{tables/metaFeatureTimes}

\subsection{Further Insights}

Further insights to be noted are the accuracies predicted for classifiers by the regression models, and the static ranking constructed by the best algorithm baseline. The accuracy of the predictions achieved by the regression-based rankers are depicted in Table \ref{tab:rootMeanSquareError}, for full and reduced meta data respectively. The first thing that attracts attention in this table is the absurdly high values for the LinearRegression and M5P rankers, however, these are all caused by the very high maximum value on one data set with id 685 on OpenML. If these extreme maxima remain out of consideration for the LinearRegression and M5P ranker (the other algorithms do not have this problem) more reasonable values are obtained. For the M5P ranker, on the full meta data the new maximum is 311.526, mean 8.127 and standard deviation 18.407; on the reduced meta data it is a maximum of 474.692, mean 15.444 and standard deviation 27.433. For the LinearRegression ranker, on the full meta data the new maximum is 142.996, mean 9.281 and standard deviation 8.438; on the reduced meta data it is a maximum of 131.456, mean 14.351 and standard deviation 10.299. However, these values are still worse than the ones obtained for the RandomForest and REPTree variants. It comes as little surprise that the RandomForest ranker also delivers the most accurate results for performance estimates, but as these best values are more than 6\% off on average for the full meta data and even more than 10\% off on average for the reduced meta data, it is questionable whether these prediction can be useful. This is especially the case, since in addition to a relatively high mean, the standard deviation is rather high as well in all observed variants. The usefulness of the tool to predict performance values is therefore dependent on the task these predictions would be utilized for and whether it requires very accurate predictions of performance values. On the other hand, judging by the fact that the regression-based approach shows to be superior to the preference-based approach, these imperfect predictions seem still seem to be accurate enough to result in a rather good ranking.

\input{tables/rootMeanSquareError}

The second interesting thing to consider is the ranking which is created by the best algorithm baseline. This ranking constitutes meta-knowledge about the learning process itself in that it shows which algorithms are generally a good choice for many data sets. This ranking is depicted in Table \ref{tab:bestAlgorithmRanking}. The table is to be read top to bottom; of the 22 algorithms, the algorithm in the i-th row is placed first on most data sets when compared to the next 22-i algorithms. The associated number represents the number of data sets on which the algorithm was placed first. It is, however, not a particularly surprising result, as for example the placement of random forest as the first algorithm, and ZeroR in the lower ranks was to be expected. Random forest is a learning algorithm that is known to often deliver good performances, which was proven again in the results discussed previously - out of the reviewed ranking approaches, the RandomForest ranker was the best choice. On the other hand, ZeroR is a simple classifier often used as a baseline, that predicts a static value based on the majority class (or average numeric value of the target feature) \cite{devasena2011effectiveness}.

\input{tables/bestAlgorithmRanking}