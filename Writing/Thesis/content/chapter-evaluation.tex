% !TEX root = ../my-thesis.tex
%
\chapter{Evaluation}
\label{sec:evaluation}

\section{Experimental Setup}

% Gathering data sets from OpenML

% Generating base performance data

% Oracle

% Best Algo Performance
Both variants compared against Best Algorithm baseline, which was implemented as a Ranker of type PreferenceRanker itself and evaluated the same way as the other PreferenceRankers. This baseline is computed iteratively by training an oracle on the same data set and then 

% Evaluation Measures used & computed

% How find best version of preference and regression ranker

\section{Results}

% Results of Base Algorithms and full meta features
% - scatter plot dataset -> best preferenceRanker, best regressionRanker, bestAlgorithmbaseline each Kendall

% Results include the 4 evals on all sets, how long predictions took. Meta feature calc times sperately, but may give avg of comp+predict time each time meta feature values table in appendix with mean,min,max stdev

% When removing probing

% When removing expensive meta features

% Results of optimized variants? full / no probing / no expensive

% Insights about classifiers: place best ranking, min, max rank (optional) for each classif?