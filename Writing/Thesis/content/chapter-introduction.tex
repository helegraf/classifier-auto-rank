% Move from general to specific
\chapter{Introduction}
\label{sec:intro}

%% Context
% Need more automated analysis of data
The potential of big data is evident, and an increasing amount of information is collected and available for analysis - but this potential is not utilized. In a white paper, the International Data Corporation claims that in 2012, out of the 2.8 zettabytes (ZB) of available data only 3\% were tagged as to enable further processing, and only 0.5\% were analyzed \cite{gantz2012the}. A follow-up paper in 2017 projects that in 2025, 15\% of the estimated 163ZB of global data will be tagged, and approximately 3\% analyzed \cite{gantz2017data}. While this is more optimistic, it still shows that there is a huge gap between the amount of data that could potentially be used and the amount of data actually available. This indicates that the demand of data to be analyzed cannot be covered by data scientists alone, and the process is not accessible enough to non-experts. It thus calls for automation of the process in a way that not much expertise in the field of machine learning is needed to gain insights about the collected data.

% Of ML, tasks, classification is important & classifier performances vary across data sets, so it is not easy to just pick one
One of the most prominent machine learning tasks is classification: A class is assigned to an instance, for example clients of a bank may be either deemed creditworthy or not, based on factors like other existing credits or the job of the client. But selecting a fitting classifier for a new data set is difficult, since algorithm performances can vary substantially among data sets, and it is not feasible to simply apply a large number of them to empirically find a good match. For example, on a data set about the electricity prices in the Australian state New South Wales \cite{harris1999splice}, the predictive accuracy for the Multilayer Perceptron\footnote{With standard hyperparameters (L:0.3,M:0.2,N:500,V:0,S:0,E:20,H:a).} is 0.7887 \cite{cachada2017run3}. The predictive accuracy of the Random Forest\footnote{With standard hyperparameters (P:100,I:100,num-slots:1,K:0,M:1.0,V:0.001,S:1).} algorithm on the same data set is 0.9236 \cite{cachada2017run}, a much higher value. On a different data set, with the topic of vehicle silhouettes \cite{siebert1987vehicle}, we get a predictive accuracy of 0.7979 for the Multilayer Perceptron \cite{cachada2017run4}, and 0.7518 for Random Forests \cite{cachada2017run2}, showing an advantage of the former on this data set\footnote{Hyperparameters as above.}. So in each case, one would have picked a different algorithm in order to achieve the best results, and this example just illustrates the choice between two algorithms - in reality, the number of available algorithms is much larger. In general, this means that for a different data set, a different algorithm might yield the best performance.

\section{Problem Statement}
\label{sec:intro:problem}
%% Problem + Significance
Since there is no one best classifier for all data sets, it can be concluded that how well a classifier performs on a given data set is dependent on certain properties of the data set. Combined with the need for automated machine learning and the importance but difficulty of selecting a fitting classification algorithm, this calls for an approach that exploits properties of data sets to automatically suggest well-performing classifiers for a new problem.

%% Response
% Therefore the goal is testing the this assumption if possible by using regression algos and preference learning
Thus, the aim of this thesis is the implementation and evaluation of two different approaches to ranking classification algorithms based on past-performances of the algorithms and according to properties of the data set. The two approaches include one regression-based approach that breaks down the problem of predicting a ranking into predicting a single performance value for each algorithm and then ranking them accordingly, and a preference based approach that is concerned with learning the rankings directly. Especially in the case of regression based ranking there is reason to believe that such a prediction is possible, as regression models have been used successfully to predict the performance of an algorithm dependent on the hyperparameter configuration \cite{DBLP:conf/aaai/EggenspergerHHL15}.

\section{Objectives}
\label{sec:intro:objectives}

The thesis has two main goals. The first is to investigate the question whether it is possible to learn rankings of classification algorithms based on certain properties of data sets. This question is explored to the extent of empirically determining whether one of the proposed approaches has an advantage over a static baseline that does not use these properties for the prediction of a ranking. Furthermore, it is assessed how well both approaches perform in comparison to the correct ranking of classifiers for a data set. The second main goal is to compare the two different approaches among each other.

In order to conduct this evaluation, an implementation of the two approaches is needed. A third goal is thus the development of an AutoML-tool that, after a pre-processing phase, returns a ranking of classifiers for a given data set, using either regression- or preference models for the prediction. The pre-processing phase constitutes of recording classifier performances on a number of data sets together with properties of the data sets to serve as training data, and training the regression models or preference model respectively.

\section{Thesis Structure}
\label{sec:intro:structure}
%% Roadmap
The following paragraphs give an outline of the thesis structure by providing a brief summary of each chapter.

\textbf{Chapter \ref{sec:fundamentals} - \nameref{sec:fundamentals}} \\[0.2em]
% Begin with fundamentals to give a brief overview of the field of ML relevant to this thesis
First, some preliminaries are discussed. A brief overview of tasks from the field of machine learning which are relevant to this thesis is given, and methods used for evaluation are explained. 

\textbf{Chapter \ref{sec:approach} - \nameref{sec:approach}} \\[0.2em]
% Continue with description of the approach to the problem described in detail
The next chapter follows up by describing the approach of this thesis for ranking classification algorithms. The two different proposed methods are contrasted. Following the details of the approach, the implementation thereof is presented.

% How the conducted experiments have been set up and discussion of results
\textbf{Chapter \ref{sec:evaluation} - \nameref{sec:evaluation}} \\[0.2em]
The evaluation of the different ranking implementations is described by first clarifying the experimental setup used for the evaluation, followed by a discussion of the results.

% Visit more work
\textbf{Chapter \ref{sec:related} - \nameref{sec:related}} \\[0.2em]
After the approach of this thesis has been laid out in detail, the scope is extended to related work in the area of AutoML in general and ranking of learning algorithms more specifically. Differences and similarities in the approaches are discussed briefly.

\textbf{Chapter \ref{sec:conclusion} - \nameref{sec:conclusion}} \\[0.2em]
% Conlude + future work
In the last chapter, the results which have been achieved are revisited with the goals in mind. Lastly, future work is outlined.

