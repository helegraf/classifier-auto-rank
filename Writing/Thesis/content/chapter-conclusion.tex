% Move from specific to general
% Use existing literature for confirmation, contradiction, comparison
% "Speak to introduction"
\chapter{Conclusion}
\label{sec:conclusion}
% Introductory restatement of research problems, aims / reseach question -> remin of problem + purpose and how addressed
In this thesis, the problem of predicting a ranking of classification algorithms for a new data set on the basis of meta features of the data set and past performances of the algorithms has been considered. Being able to predict a ranking of such algorithms is desirable since this potentially speeds up and simplifies the process of algorithm selection, which is relevant due to a rapidly increasing amount of available data, and more importantly, data that is available but has not yet been analyzed. This problem has been addressed by implementing two different approaches, regression-based and preference-based ranking. Both have been evaluated extensively against a baseline and an oracle.

% Summary of findings and limitations: what has been covered LIMITATION: SMALL DATA SETS ESP!!
The evaluation showed a significant advantage regarding Kendall's rank correlation coefficient for all tested methods in comparison to the best algorithm baseline, except instance based label ranking in the standard configuration and label ranking by pairwise comparison for the full meta data. However, regarding the practically possibly more important measure of best three loss, only the three regression based variants with linear regression, M5P and random forest as the regression learners can hold a significant advantage. When evaluating best three loss on reduced meta data that does not incorporate probing, although the evaluated data indicates a slight advantage of those same regression models over the baseline still, it is not statistically significant anymore. Furthermore, some label ranking methods even show a significant disadvantage in comparison to the baseline for the best three loss and loss for both evaluation on the full and reduced meta data. Thus it can be concluded that in general, a regression-based approach seems to be more promising than a preference-based approach, but is not better than the baseline regarding the best three loss when evaluated on the selected meta data without probing. Out of the regression-based variants, random forest seems to be the most promising one.

The main limitation of these findings is that the data sets used in the evaluation process were not very large, being limited to a maximum of 100 features and 1000 instances. Often, data sets can get much larger and the question whether this ranking approach would then still pose an efficient solution regarding the calculation of meta features and the resulting prediction times and, maybe even more importantly, an accurate enough solution for predicting classifiers, remains. 

% Practical applications / limitations: assess value / relevance / implications: What does it mean for theory, what for practice
On the basis of these results, it can be concluded that there is an indication of a connection between certain meta features of a data set and the predictive accuracy of classification algorithms for this data set, which can be exploited to a degree by regression models and label ranking models to predict a ranking of classification algorithms. A practical application of these findings, apart from the direct use as a ranker, may be to incorporate the implementation or parts of it in another AutoML tool as a search heuristic, similar to how some AutoML solutions like AUTO-SKLEARN already benefits from meta learning \cite{feurer2015efficient}. However, some additional work may have to be done in extension to this thesis for such an integration to be sensible.

% Recommendations for future work
\section{Future Work}
\label{sec:conclusion:future}
% More evaluation - meta features
As the results have shown that the approach taken in this thesis has worked out reasonably well, for example when considering the regression-based approach using random forest, many possibilities for additional work arise. An intuitive thought would be to extend the evaluation of the tool, for instance in terms of meta features. Additional sets of meta features could be evaluated, like adding implementations for additional meta features, or considering using only light probing by incorporating the performance values for usually cheap algorithms in the meta features. Since the significant advantage of any approach was lost regarding best three loss when moving from the full meta data to the reduced meta data without probing, this might be especially interesting. One could consider adding the meta feature groups for DecisionStump, RandomTreeDepth2, REPTreeDepth2, and J48.0001, which would add approximately 12 milliseconds to the already existing 3 milliseconds of calculation time for the meta features in the mean, which seemss reasonable. An effect on the time and quality of predictions for larger data sets would have to be examined still.

% More evaluation - loss - time & compare agains Jan van Rijn
Furthermore, more insights could be gained about whether this approach is competitive by comparing it with other ranking approaches, for example like the one proposed by \citeauthor{DBLP:journals/ml/AbdulrahmanBRV18} \cite{DBLP:journals/ml/AbdulrahmanBRV18}. In this regard it would also be interesting  to implement the loss- and time-loss curves proposed by the authors, which show how loss decreases as one advances in the predicted ranking and tries out the recommended algorithms, and at what cost in the sense of time the sampling of algorithms comes respectively.

% Use this for predicting different algorithms
Additionally, so far the evaluation of the tool has been confined to predicting algorithms for classification. Since this has been relatively successful, it could be tested whether similar predictions can also be made for other machine learning tasks like regression or clustering with the same approach by extending the tool to handling these functions. Also, one could try to include a few parameterized versions of algorithms, as so far only one variant of each classifier with standard hyperparameters has been used. 

% Use this to predict other measure
In a similar manner to extending the algorithms for which a performance measure is predicted, one could use the tool to predict different performance measures. For this, the tool does not even need to be adapted, as it is agnostic to the semantics behind the measure it predicts, it only needs to be numeric. One possibility for this would be the prediction of build or prediction times of learning algorithms, or a measure that combines time and accuracy like A3R \cite{DBLP:journals/ml/AbdulrahmanBRV18}.

% Auto fitting
Depending on the fact whether one is willing to trade off longer build and prediction times for improved accuarcy, it would also be possible to fit more sophisticated regression models for the regression based approach. This could include feature pre-processing, automating the choice of regression algorithm with one of the available tools \cite{thornton2013auto} \cite{feurer2015efficient}, or even using a whole pipeline prediction tool \cite{wever2017automatic} \cite{DBLP:conf/eurogp/SaPOP17}. 

% Additional baseline
Last, it could be interesting to implement an additional baseline that averages ranks instead of counting how many times an algorithm is ranked first. This would help to determine if the ranking returned by the best algorithm baseline (see Fig. \ref{tab:bestAlgorithmRanking}) still holds when using a different baseline. Naturally, this new baseline then could also be used to re-evaluate the implemented ranking concepts. 


