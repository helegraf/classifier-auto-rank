% Move from specific to general
% Use existing literature for confirmation, contradiction, comparison
% "Speak to introduction"
\chapter{Conclusion}
\label{sec:conclusion}
% Introductory restatement of research problems, aims / reseach question -> remin of problem + purpose and how addressed
In this thesis, the problem of predicting a ranking of classification algorithms for a new data set on the basis of meta features of the data set and past performances of the algorithms has been considered. Being able to predict a ranking of such algorithms is desirable since this potentially speeds up and simplifies the process of algorithm selection, which is important due to a rapidly increasing amount of available data, and more importantly, data that is available but has not yet been analyzed. This problem has been addressed by implementing two different approaches, regression-based and preference-based ranking. Both have been evaluated extensively against a baseline and an oracle.

% Summary of findings and limitations: what has been covered LIMITATION: SMALL DATA SETS ESP!!
- Results - 

% Practical applications / limitations: assess value / relevance / implications: What does it mean for theory, what for practice
On the basis of these results, it can be concluded that a causal connection exists between certain meta features of a data set and the predictive accuracy of classification algorithms for this data set, which can be exploited to a degree by regression models and label ranking models to predict a ranking of classification algorithms. A practical application of these findings, apart from the direct use as a ranker, may be to incorporate the implementation or parts of it in another Auto-ML tool as a search heuristic, similar to how some Auto-ML solutions like AUTO-SKLEARN already benefits from meta learning \cite{feurer2015efficient}. However, some additional work may have to be done in extension to this thesis in order for a sensible integration.

% Recommendations for future work
\section{Future Work}
\label{sec:conclusion:future}
% More evaluation - meta features
As the results have shown that the approach taken in this thesis has worked out reasonably well, for example when considering the regression based approach using random forests, many possibilities for additional work arise. An intuitive thought would be to extend the evaluation of the tool, for instance in terms of meta features. Different sets of meta features could be evaluated, like adding implementations for additional meta features, or considering using only light probing by incorporating the performance values for usually cheap algorithms in the meta features. 

% More evaluation - loss - time & compare agains Jan van Rijn
Furthermore, more insights could be gained about whether this approach is competitive by comparing it with other ranking approaches, for example like the one proposed by \citeauthor{DBLP:journals/ml/AbdulrahmanBRV18} \cite{DBLP:journals/ml/AbdulrahmanBRV18}. In this regard it would also be interesting  to implement the loss - and time - loss curves proposed by the authors, which show how loss decreases as one advances in the predicted ranking and tries out the recommended algorithms, and at what cost in the sense of time the sampling of algorithms comes respectively.

% Use this for predicting different algorithms
Additionally, so far the evaluation of the tool has been confined to predicting algorithms for classification. Since this has been relatively successful, it could be tested whether similar predictions can also be made for other machine learning tasks like regression or clustering with the same approach by extending the tool to handling these functions. Also, one could try to include a few parameterized versions of algorithms, as so far only one variant of each classifier with standard hyperparameters has been used. 

% Use this to predict other measure
In a similar manner to extending the algorithms for which a performance measure is predicted, one could use the tool to predict different performance measures. For this, the tool does not even need to be adapted, as it is agnostic to the semantics behind the measure it predicts, it only needs to be numeric. One possibility for this would be the prediction of build or prediction times of learning algorithms, or a measure that combines time and accuracy \cite{DBLP:journals/ml/AbdulrahmanBRV18}.

% Auto fitting
Depending on the fact whether one is willing to trade-off longer build and prediction times, it would also be possible to fit more sophisticated regression models for the regression based approach. This could include feature pre-processing, automating the choice of regression algorithm with one of the available tools \cite{thornton2013auto} \cite{feurer2015efficient}, or even using a whole pipeline prediction tool \cite{wever2017automatic} \cite{DBLP:conf/eurogp/SaPOP17}. Furthermore, the building and predictions for the regression models could be parallelized and evaluated on a cluster to simulate and test  the usage of the ranking tool in a productive environment.

% Additional baseline
Last, it could be interesting to implement an additional baseline that averages ranks instead of counting how many times an algorithm is ranked first. This would help to determine if the surprising ranking returned by the best algorithm baseline (see Fig. \ref{tab:bestAlgorithmRanking}
) still holds when using a different baseline. Naturally this new baseline then could also be used to re-evaluate the implemented ranking concepts. 


