% !TEX root = ../my-thesis.tex
%
\chapter{Approach}
\label{sec:approach}

Approach of ranking classifier implementations here realized by two concepts, regression-based and preference-based ranking, which will be explained in detail. Also predicting predictive accuracy here. Thus target function of the form $meta features^n \rightarrow classifiers^m$.
performance measure chosen predictive accuracy

% Theory of approach
Since the aim is to generate a ranking of classification algorithms, first, performance values of a number of classifiers are recorded on a few data sets. Then, meta features are computed for each data set. This is for training the ranking model. This generates a table that contains all necessary informatino for training both types of rankers. A ranking of the classifiers for a new instance is then achieved by computing the meta features for the data set and subsequently uing it to query the underlying model. 

\begin{table}[h]
\centering
	\begin{tabularx}{\textwidth}{X | X | X | X | X | X | X | X}
		%\hline
		MF 1			& MF 2		& ... 	& MF n		& PV 1 		& PV 2 		&	...	&	PV k 		\\ \hline
		$v_{11}$		& $v_{12}$	& ...	& $v_{1n}$	& $p_{11}$	& $p_{12}$	& 	...	&	$p_{1k}$		\\ \hline
		$v_{21}$		& $v_{22}$	& ...	& $v_{2n}$	& $p_{21}$	& $p_{22}$	& 	...	&	$p_{2k}$		\\ \hline
		...			& ...		& ...	& ...		& ...		& ...		&	...	&	...			\\ \hline
		$v_{m1}$		& $v_{m2}$	& ... 	& $v_{mn}$	& $p_{2m}$	& $p_{m2}$	& 	...	&	$p_{mk}$			 
	\end{tabularx}
	\label{tab:table1}
	\caption{This is a caption text.}
\end{table}

% How is the regression ranking done
One possibility to derive a ranking of classifiers from this information when given a new data set is to use regression models. The idea is to use separate regression models to predict a performance value for each classifier, and to then derive an ordering from these predictions. This is done by splitting the data set compiled beforehand into separate data sets that each contain all meta features for all data sets, but only the performance values of one classifier. The target feature on these individual data sets, the performance value of the classifier, is a numeric value, and thus a regression model can be trained on each of them. All regression models therefore try to learn the target function $meta features \rightarrow classifier performance value$ for their respective classifier. Hence for a query instance, after computing the meta features, these are fed into each regression model and the predicted performance value for each classifier is saved. In a second step, the classifiers are ordered according to the predictions.

\begin{table}[h]
\centering
	\begin{tabularx}{\textwidth}{X | X | X | X}
		%\hline
		Meta Feature 1	& ... 	& Meta Feature n		& Performance Value 1	\\ \hline
		$v_{11}$			& ...	& $v_{1n}$			& $p_{11}$				\\ \hline
		$v_{21}$			& ...	& $v_{2n}$			& $p_{21}$				\\ \hline
		...				& ...	& ...				& ...					\\ \hline
		$v_{m1}$			& ... 	& $v_{mn}$			& $p_{m1}$				\\ \hline\hline
		Meta Feature 1	& ... 	& Meta Feature n		& Performance Value 2 	\\ \hline
		$v_{11}$			& ...	& $v_{1n}$			& $p_{12}$				\\ \hline
		$v_{21}$			& ...	& $v_{2n}$			& $p_{22}$				\\ \hline
		...				& ...	& ...				& ...					\\ \hline
		$v_{m1}$			& ... 	& $v_{mn}$			& $p_{m2}$					 
	\end{tabularx}
	
\vspace{1em}
...
\vspace{1em}	
	
	\begin{tabularx}{\textwidth}{X | X | X | X}
		%\hline
		Meta Feature 1	& ... 	& Meta Feature n		& Performance Value k 	\\ \hline
		$v_{11}$			& ...	& $v_{1n}$			& $p_{1k}$				\\ \hline
		$v_{21}$			& ...	& $v_{2n}$			& $p_{2k}$				\\ \hline
		...				& ...	& ...				& ...					\\ \hline
		$v_{m1}$			& ... 	& $v_{mn}$			& $p_{mk}$					 
	\end{tabularx}
	
	\label{tab:table1}
	\caption{This is a caption text.}
\end{table}

% How is the preference ranking done
The second possibility considered in this context is using preference learning to predict a ranking. Each instance of the data set generated beforehand contains meta feature information for the considered data set and performance values of all classifiers. Similarly to the regression alternative, the performance values can be converted to preference information by sorting the corresponding classifiers by their performance values. This leads to an ordering of classifiers associated with each instance, and implies that the preference learning task at hand is label ranking. In this case, there exists only one label ranking mode which is fed the computed meta data for a new data set and returns an ordering of classifiers. Thereby it attempts to learn the mapping from meta features to an ordering of classifiers directly.

\begin{table}[h]
\centering
	\begin{tabularx}{\textwidth}{X | X | X | X | X | X | X | X}
		%\hline
		MF 1				& MF 2				& ... 	& MF n				& PV 1 		& PV 2 		&	...	&	PV k 		\\ \hline
		$v_{11}$			& $v_{12}$			& ...	& $v_{1n}$			& $p_{11}$	& $p_{12}$	& 	...	&	$p_{1k}$		\\ \hline
		$v_{21}$			& $v_{22}$			& ...	& $v_{2n}$			& $p_{21}$	& $p_{22}$	& 	...	&	$p_{2k}$		\\ \hline
		...				& ...				& ...	& ...				& ...		& ...		&	...	&	...			\\ \hline
		$v_{m1}$			& $v_{m2}$			& ... 	& $v_{mn}$			& $p_{2m}$	& $p_{m2}$	& 	...	&	$p_{mk}$			 
	\end{tabularx}
	\label{tab:table1}
	\caption{This is a ranking}
\end{table}

META FEATURES USED!


for mann-whitney U apache commons implementation used with ties averaged and NaNs removed before